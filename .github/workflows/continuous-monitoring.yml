name: Continuous CI/CD Monitoring

on:
  schedule:
    # Run every 15 minutes for continuous monitoring
    - cron: '*/15 * * * *'
  workflow_dispatch:
    inputs:
      alert_threshold:
        description: 'Alert threshold for failures'
        required: false
        default: '2'
      monitoring_window:
        description: 'Monitoring window in hours'
        required: false
        default: '24'

env:
  ALERT_THRESHOLD: ${{ github.event.inputs.alert_threshold || '2' }}
  MONITORING_WINDOW: ${{ github.event.inputs.monitoring_window || '24' }}
  PYTHON_VERSION: '3.12'

jobs:
  health-check:
    name: CI/CD Health Check
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install monitoring dependencies
      run: |
        pip install --upgrade pip
        pip install PyGithub requests aiohttp pandas matplotlib seaborn
    
    - name: Check CI/CD Pipeline Health
      id: health-check
      run: |
        python -c "
        import os
        import json
        import subprocess
        from datetime import datetime, timedelta
        
        # Get recent workflow runs
        result = subprocess.run(
            ['gh', 'run', 'list', '--json', 'status,conclusion,name,createdAt', '--limit', '50'],
            capture_output=True, text=True
        )
        
        if result.stdout:
            runs = json.loads(result.stdout)
            
            # Calculate metrics
            total_runs = len(runs)
            failed_runs = sum(1 for r in runs if r.get('conclusion') == 'failure')
            success_rate = ((total_runs - failed_runs) / total_runs * 100) if total_runs > 0 else 0
            
            # Check for recent failures
            recent_failures = []
            cutoff = datetime.now() - timedelta(hours=float(os.environ['MONITORING_WINDOW']))
            
            for run in runs:
                created_at = datetime.fromisoformat(run['createdAt'].replace('Z', '+00:00'))
                if created_at > cutoff and run.get('conclusion') == 'failure':
                    recent_failures.append({
                        'name': run['name'],
                        'time': run['createdAt']
                    })
            
            # Set outputs
            print(f'::set-output name=success_rate::{success_rate:.1f}')
            print(f'::set-output name=failed_count::{failed_runs}')
            print(f'::set-output name=recent_failures::{len(recent_failures)}')
            
            # Generate health report
            health_status = 'healthy' if success_rate > 90 else 'degraded' if success_rate > 70 else 'critical'
            print(f'::set-output name=health_status::{health_status}')
            
            # Alert if threshold exceeded
            if len(recent_failures) >= int(os.environ['ALERT_THRESHOLD']):
                print(f'::warning::CI/CD Health Alert: {len(recent_failures)} failures in last {os.environ[\"MONITORING_WINDOW\"]}h')
                for failure in recent_failures[:5]:
                    print(f'  - {failure[\"name\"]} at {failure[\"time\"]}')
        "
    
    - name: Calculate DORA Metrics
      id: dora-metrics
      run: |
        python -c "
        import subprocess
        import json
        from datetime import datetime, timedelta
        
        try:
            # Deployment Frequency
            git_log = subprocess.run(
                ['git', 'log', '--since=7 days ago', '--oneline'],
                capture_output=True, text=True
            )
            deployments = len(git_log.stdout.strip().split('\\n')) if git_log.stdout.strip() else 0
            deployment_frequency = deployments / 7
            
            # Lead Time (using PR data)
            pr_result = subprocess.run(
                ['gh', 'pr', 'list', '--state', 'merged', '--limit', '10', '--json', 'mergedAt,createdAt'],
                capture_output=True, text=True
            )
            
            lead_time_hours = 24  # default
            if pr_result.stdout:
                prs = json.loads(pr_result.stdout)
                if prs:
                    lead_times = []
                    for pr in prs:
                        if pr.get('mergedAt') and pr.get('createdAt'):
                            created = datetime.fromisoformat(pr['createdAt'].replace('Z', '+00:00'))
                            merged = datetime.fromisoformat(pr['mergedAt'].replace('Z', '+00:00'))
                            lead_times.append((merged - created).total_seconds() / 3600)
                    if lead_times:
                        lead_time_hours = sum(lead_times) / len(lead_times)
            
            # Change Failure Rate
            ci_result = subprocess.run(
                ['gh', 'run', 'list', '--workflow', 'CI', '--limit', '20', '--json', 'conclusion'],
                capture_output=True, text=True
            )
            
            change_failure_rate = 0
            if ci_result.stdout:
                ci_runs = json.loads(ci_result.stdout)
                if ci_runs:
                    failures = sum(1 for r in ci_runs if r.get('conclusion') == 'failure')
                    change_failure_rate = (failures / len(ci_runs)) * 100
            
            # MTTR (simplified)
            mttr_minutes = 30  # baseline
            
            print(f'::set-output name=deployment_frequency::{deployment_frequency:.2f}')
            print(f'::set-output name=lead_time_hours::{lead_time_hours:.1f}')
            print(f'::set-output name=change_failure_rate::{change_failure_rate:.1f}')
            print(f'::set-output name=mttr_minutes::{mttr_minutes}')
            
            # Performance level
            if deployment_frequency >= 1 and lead_time_hours <= 24 and change_failure_rate <= 15:
                performance = 'Elite'
            elif deployment_frequency >= 0.14 and lead_time_hours <= 168 and change_failure_rate <= 30:
                performance = 'High'
            elif deployment_frequency >= 0.03 and lead_time_hours <= 720 and change_failure_rate <= 45:
                performance = 'Medium'
            else:
                performance = 'Low'
            
            print(f'::set-output name=performance_level::{performance}')
            
        except Exception as e:
            print(f'Error calculating DORA metrics: {e}')
        "
    
    - name: Analyze Test Stability
      run: |
        python -c "
        import subprocess
        import json
        import re
        from collections import defaultdict
        
        # Get recent test results
        try:
            # Look for test failure patterns
            result = subprocess.run(
                ['gh', 'run', 'list', '--workflow', 'CI', '--status', 'failure', '--limit', '10', '--json', 'databaseId'],
                capture_output=True, text=True
            )
            
            if result.stdout:
                failed_runs = json.loads(result.stdout)
                
                flaky_tests = defaultdict(int)
                failure_patterns = defaultdict(int)
                
                for run in failed_runs[:5]:  # Analyze top 5 failures
                    # Get run logs (simplified - would need proper API call)
                    run_id = run['databaseId']
                    
                    # Categorize failures
                    failure_patterns['test_failures'] += 1
                
                print('## Test Stability Analysis')
                print(f'Recent test failures analyzed: {len(failed_runs)}')
                
                if failure_patterns:
                    print('\\nFailure Categories:')
                    for category, count in failure_patterns.items():
                        print(f'  - {category}: {count}')
                        
        except Exception as e:
            print(f'Could not analyze test stability: {e}')
        "
    
    - name: Check Security Vulnerabilities
      continue-on-error: true
      run: |
        # Check for security issues in dependencies
        pip install safety
        safety check --json > security-report.json || true
        
        python -c "
        import json
        try:
            with open('security-report.json') as f:
                report = json.load(f)
                vulnerabilities = report.get('vulnerabilities', [])
                
                if vulnerabilities:
                    print(f'::warning::Found {len(vulnerabilities)} security vulnerabilities')
                    for vuln in vulnerabilities[:3]:
                        print(f'  - {vuln.get(\"package\", \"unknown\")}: {vuln.get(\"vulnerability\", \"\")}')
                else:
                    print('âœ… No security vulnerabilities found')
        except:
            print('Could not parse security report')
        "
    
    - name: Performance Monitoring
      run: |
        python -c "
        import subprocess
        import json
        from datetime import datetime, timedelta
        
        # Monitor build times
        result = subprocess.run(
            ['gh', 'run', 'list', '--workflow', 'CI', '--limit', '20', '--json', 'createdAt,updatedAt,status'],
            capture_output=True, text=True
        )
        
        if result.stdout:
            runs = json.loads(result.stdout)
            
            build_times = []
            for run in runs:
                if run.get('status') == 'completed' and run.get('createdAt') and run.get('updatedAt'):
                    start = datetime.fromisoformat(run['createdAt'].replace('Z', '+00:00'))
                    end = datetime.fromisoformat(run['updatedAt'].replace('Z', '+00:00'))
                    duration = (end - start).total_seconds() / 60
                    build_times.append(duration)
            
            if build_times:
                avg_build_time = sum(build_times) / len(build_times)
                max_build_time = max(build_times)
                min_build_time = min(build_times)
                
                print(f'## Build Performance Metrics')
                print(f'Average build time: {avg_build_time:.1f} minutes')
                print(f'Fastest build: {min_build_time:.1f} minutes')
                print(f'Slowest build: {max_build_time:.1f} minutes')
                
                if avg_build_time > 10:
                    print(f'::warning::Build times exceeding 10 minutes - consider optimization')
        "
    
    - name: Generate Monitoring Report
      run: |
        cat > monitoring-report.md << EOF
        # CI/CD Monitoring Report
        Generated: $(date)
        
        ## Health Status: ${{ steps.health-check.outputs.health_status }}
        
        ### Key Metrics
        - **Success Rate**: ${{ steps.health-check.outputs.success_rate }}%
        - **Failed Runs**: ${{ steps.health-check.outputs.failed_count }}
        - **Recent Failures**: ${{ steps.health-check.outputs.recent_failures }}
        
        ### DORA Metrics
        - **Deployment Frequency**: ${{ steps.dora-metrics.outputs.deployment_frequency }} per day
        - **Lead Time**: ${{ steps.dora-metrics.outputs.lead_time_hours }} hours
        - **Change Failure Rate**: ${{ steps.dora-metrics.outputs.change_failure_rate }}%
        - **MTTR**: ${{ steps.dora-metrics.outputs.mttr_minutes }} minutes
        - **Performance Level**: ${{ steps.dora-metrics.outputs.performance_level }}
        
        ### Recommendations
        EOF
        
        # Add recommendations based on metrics
        if [ "${{ steps.health-check.outputs.health_status }}" = "critical" ]; then
          echo "- ðŸš¨ URGENT: CI/CD system in critical state - immediate attention required" >> monitoring-report.md
        fi
        
        if (( $(echo "${{ steps.dora-metrics.outputs.change_failure_rate }} > 20" | bc -l) )); then
          echo "- âš ï¸ High failure rate detected - review test stability" >> monitoring-report.md
        fi
        
        if (( $(echo "${{ steps.dora-metrics.outputs.deployment_frequency }} < 0.5" | bc -l) )); then
          echo "- ðŸ“ˆ Consider increasing deployment frequency" >> monitoring-report.md
        fi
    
    - name: Upload Monitoring Report
      uses: actions/upload-artifact@v4
      with:
        name: ci-cd-monitoring-report-${{ github.run_number }}
        path: monitoring-report.md
        retention-days: 30
    
    - name: Send Alerts (if needed)
      if: steps.health-check.outputs.health_status == 'critical'
      run: |
        echo "::error::CI/CD System Critical - Immediate Action Required"
        
        # Would send webhook alert here
        python -c "
        import json
        
        alert = {
            'severity': 'critical',
            'title': 'CI/CD System Health Alert',
            'status': '${{ steps.health-check.outputs.health_status }}',
            'metrics': {
                'success_rate': '${{ steps.health-check.outputs.success_rate }}%',
                'recent_failures': '${{ steps.health-check.outputs.recent_failures }}',
                'performance_level': '${{ steps.dora-metrics.outputs.performance_level }}'
            },
            'timestamp': '$(date -Iseconds)'
        }
        
        print('Alert payload:', json.dumps(alert, indent=2))
        
        # In production, would POST to webhook URL
        # requests.post(webhook_url, json=alert)
        "

  predictive-analysis:
    name: Predictive Failure Analysis
    runs-on: ubuntu-latest
    needs: health-check
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Run Predictive Analysis
      run: |
        python -c "
        import subprocess
        import json
        from datetime import datetime, timedelta
        import hashlib
        
        # Simplified predictive analysis
        try:
            # Get recent commits
            result = subprocess.run(
                ['git', 'log', '--since=1 day ago', '--format=%H|%ae|%s', '--max-count=10'],
                capture_output=True, text=True
            )
            
            if result.stdout:
                commits = result.stdout.strip().split('\\n')
                
                high_risk_commits = []
                
                for commit in commits:
                    if '|' in commit:
                        sha, author, message = commit.split('|', 2)
                        
                        risk_score = 0
                        risk_factors = []
                        
                        # Risk analysis
                        if 'breaking' in message.lower():
                            risk_score += 0.3
                            risk_factors.append('breaking change')
                        
                        if 'dependency' in message.lower() or 'deps' in message.lower():
                            risk_score += 0.25
                            risk_factors.append('dependency change')
                        
                        if 'refactor' in message.lower():
                            risk_score += 0.2
                            risk_factors.append('refactoring')
                        
                        if risk_score > 0.4:
                            high_risk_commits.append({
                                'sha': sha[:8],
                                'risk_score': risk_score,
                                'factors': risk_factors
                            })
                
                if high_risk_commits:
                    print('## High Risk Commits Detected')
                    for commit in high_risk_commits:
                        print(f'- {commit[\"sha\"]}: Risk {commit[\"risk_score\"]:.1%} ({', '.join(commit[\"factors\"])})')
                    print('\\n::warning::High-risk commits detected - monitor CI closely')
                else:
                    print('âœ… No high-risk commits detected')
                    
        except Exception as e:
            print(f'Predictive analysis error: {e}')
        "

  update-dashboard:
    name: Update Monitoring Dashboard
    runs-on: ubuntu-latest
    needs: [health-check, predictive-analysis]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Generate Dashboard Data
      run: |
        # Create dashboard JSON for external monitoring tools
        cat > dashboard-data.json << EOF
        {
          "timestamp": "$(date -Iseconds)",
          "health_status": "${{ needs.health-check.outputs.health_status }}",
          "metrics": {
            "success_rate": "${{ needs.health-check.outputs.success_rate }}",
            "deployment_frequency": "${{ needs.health-check.outputs.deployment_frequency }}",
            "lead_time_hours": "${{ needs.health-check.outputs.lead_time_hours }}",
            "change_failure_rate": "${{ needs.health-check.outputs.change_failure_rate }}",
            "mttr_minutes": "${{ needs.health-check.outputs.mttr_minutes }}",
            "performance_level": "${{ needs.health-check.outputs.performance_level }}"
          },
          "alerts": []
        }
        EOF
        
    - name: Update Metrics Database
      continue-on-error: true
      run: |
        # Would update external metrics database here
        echo "Dashboard data generated - would sync to monitoring service"
        
    - name: Archive Dashboard Data
      uses: actions/upload-artifact@v4
      with:
        name: dashboard-data-${{ github.run_number }}
        path: dashboard-data.json
        retention-days: 90